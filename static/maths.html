<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="shortcut icon" href="/static/favicon.ico" type="image/x-icon">
  <title>ML Pipeline Math Derivations</title>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <style>
    :root {
      --primary: #4f8cff;
      --secondary: #e0f2fe;
      --accent: #fef9c3;
      --section-bg: #f5faff;
      --section-hover: #e0e7ff;
      --heading: #2563eb;
      --subheading: #0e7490;
      --border: #dbeafe;
    }
    body {
      font-family: Arial, sans-serif;
      line-height: 1.6;
      padding: 20px;
      background: linear-gradient(120deg, #e0f2fe 0%, #fef9c3 100%);
      margin: 0;
      min-height: 100vh;
    }
    h1 {
      color: var(--heading);
      margin-top: 0;
      text-align: center;
      letter-spacing: 1px;
      background: linear-gradient(90deg, #4f8cff 40%, #0ea5e9 100%);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      background-clip: text;
    }
    h2 {
      color: var(--subheading);
      margin-top: 0;
      margin-bottom: 0.7em;
      font-size: 1.3em;
      letter-spacing: 0.5px;
    }
    h3 {
      color: #64748b;
      margin-top: 1.2em;
      margin-bottom: 0.5em;
      font-size: 1.1em;
    }
    section {
      margin-bottom: 40px;
      background: var(--section-bg);
      padding: 20px;
      border-radius: 12px;
      box-shadow: 0 2px 8px rgba(79, 140, 255, 0.07);
      border: 1.5px solid var(--border);
      transition: box-shadow 0.2s, background 0.2s;
    }
    section:hover {
      box-shadow: 0 4px 18px rgba(79, 140, 255, 0.13);
      background: var(--section-hover);
    }
    code, pre {
      background-color: #fef9c3;
      padding: 2px 5px;
      border-radius: 4px;
      font-family: 'Fira Mono', 'Consolas', monospace;
      font-size: 0.97em;
      color: #334155;
    }
    ul, ol {
      padding-left: 1.2em;
    }
    ul li, ol li {
      margin-bottom: 0.4em;
    }
    .math-block {
      display: block !important;
      text-align: center;
      margin: 18px 0 !important;
      overflow-x: visible !important;
      max-width: 100%;
      width: 100%;
      background: #f0f9ff;
      border-radius: 8px;
      padding: 10px 0;
      border: 1px solid #bae6fd;
    }
    .MathJax, .mjx-chtml {
      display: inline-block !important;
      text-align: center !important;
      margin: 0 auto !important;
      overflow-x: visible !important;
      max-width: 100%;
      width: auto;
    }
    .math-inline {
      display: inline-block !important;
      vertical-align: middle !important;
    }
    /* Responsive styles */
    @media (max-width: 700px) {
      body {
        padding: 8px;
      }
      section {
        padding: 12px;
        margin-bottom: 24px;
      }
      h1 {
        font-size: 1.6em;
      }
      h2 {
        font-size: 1.15em;
      }
      h3 {
        font-size: 1em;
      }
      p, ul, ol {
        font-size: 0.98em;
      }
      .math-block, .MathJax, .mjx-chtml {
        font-size: 1em !important;
      }
    }
    @media (max-width: 480px) {
      h1 {
        font-size: 1.2em;
      }
      h2 {
        font-size: 1em;
      }
      section {
        padding: 8px;
      }
      p, ul, ol {
        font-size: 0.93em;
      }
      .math-block, .MathJax, .mjx-chtml {
        font-size: 0.95em !important;
      }
    }
    @media (max-width: 700px) {
      section:not(:last-child) {
        border-bottom: 1px solid #eee;
      }
    }
  </style>
  <script>
    // Ensure MathJax renders equations as display or inline as needed
    window.MathJax = {
      tex: {
        displayMath: [['\\[', '\\]'], ['$$', '$$']],
        inlineMath: [['\\(', '\\)'], ['$', '$']],
        processEscapes: true
      },
      options: {
        renderActions: {
          addMenu: []
        }
      }
    };
  </script>
</head>
<body>
    <!-- GitHub icon link (top right) -->
  <a href="https://github.com/anikchand461/SalaryAi" target="_blank" aria-label="GitHub Repository" style="position:fixed;top:24px;right:32px;z-index:10;display:inline-block;">
    <svg height="32" width="32" viewBox="0 0 16 16" fill="#1e293b" xmlns="http://www.w3.org/2000/svg" style="display:block;">
      <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38
      0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52
      -.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2
      -3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82a7.65
      7.65 0 0 1 2-.27c.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08
      2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01
      1.93-.01 2.19 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"/>
    </svg>
  </a>
  <h1>Mathematical Derivations of ML Pipeline</h1>

  <section>
    <h2>1. Mean Imputation (SimpleImputer)</h2>
    <p>We replace missing values (NaNs) with the mean of the column:</p>
    <p>
      Given a feature vector \( x = [x_1, x_2, \dots, x_n] \), the mean is:
    </p>
    <div class="math-block">
      \[
        \mu = \frac{1}{n} \sum_{i=1}^{n} x_i
      \]
    </div>
    <p>
      Any missing value \( x_j \) is replaced by \( \mu \).
    </p>
  </section>

  <section>
    <h2>2. One-Hot Encoding (OHE)</h2>
    <p>OHE converts categorical values to binary vectors.</p>
    <p>For categories \( C = \{c_1, c_2, c_3\} \):</p>
    <ul>
      <li>\( c_1 \rightarrow [1, 0, 0] \)</li>
      <li>\( c_2 \rightarrow [0, 1, 0] \)</li>
      <li>\( c_3 \rightarrow [0, 0, 1] \)</li>
    </ul>
  </section>

  <section>
    <h2>3. Polynomial Features</h2>
    <p>Given features \( x_1, x_2 \), a degree 2 polynomial transformation gives:</p>
    <p>\[ [1, x_1, x_2, x_1^2, x_1x_2, x_2^2] \]</p>
    <p>For degree \( d \), it includes all combinations up to \( x^d \).</p>
  </section>

  <section>
    <h2>4. Standardization</h2>
    <p>Transforms features to have mean 0 and standard deviation 1:</p>
    <p>\[ z = \frac{x - \mu}{\sigma} \]</p>
    <ul>
      <li>\( \mu \) = mean of the feature</li>
      <li>\( \sigma \) = standard deviation of the feature</li>
    </ul>
  </section>

  <section>
    <h2>5. Principal Component Analysis (PCA)</h2>
    <ol>
      <li>Center data: \( X_c = X - \bar{X} \)</li>
      <li>Covariance: \( \Sigma = \frac{1}{n} X_c^T X_c \)</li>
      <li>Eigen-decompose: \( \Sigma = V \Lambda V^T \)</li>
      <li>Project: \( X' = X_c V_k \), where \( V_k \) = top \( k \) eigenvectors</li>
    </ol>
  </section>

  <section>
    <h2>6. Polynomial Regression (OLS - Closed-form Solution)</h2>
    <p>
      In polynomial regression, we expand the features to include polynomial terms. For example, with a single feature \( x \) and degree 2, the design matrix \( X \) becomes:
    </p>
    <div class="math-block">
      \[
      X = \begin{bmatrix}
        1 & x_1 & x_1^2 \\
        1 & x_2 & x_2^2 \\
        \vdots & \vdots & \vdots \\
        1 & x_n & x_n^2
      \end{bmatrix}
      \]
    </div>
    <p>
      The model is \( y = X\beta + \epsilon \), where \( \beta = [\beta_0, \beta_1, \beta_2]^T \).
    </p>
    <p>
      The closed-form solution is:
    </p>
    <div class="math-block">
      \[
      \hat{\beta} = (X^T X)^{-1} X^T y
      \]
    </div>
    <h3>Step-by-Step Numerical Example (Degree 2)</h3>
    <p>Suppose we have:</p>
    <div class="math-block">
      \[
      x = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}, \quad
      y = \begin{bmatrix} 30000 \\ 50000 \\ 70000 \end{bmatrix}
      \]
    </div>
    <p>Construct the polynomial design matrix:</p>
    <div class="math-block">
      \[
      X = \begin{bmatrix}
        1 & 1 & 1^2 \\
        1 & 2 & 2^2 \\
        1 & 3 & 3^2
      \end{bmatrix}
      =
      \begin{bmatrix}
        1 & 1 & 1 \\
        1 & 2 & 4 \\
        1 & 3 & 9
      \end{bmatrix}
      \]
    </div>
    <p>Now, calculate \( X^T X \):</p>
    <div class="math-block">
      \[
      X^T X =
      \begin{bmatrix}
        1 & 1 & 1 \\
        1 & 2 & 3 \\
        1 & 4 & 9
      \end{bmatrix}
      \begin{bmatrix}
        1 & 1 & 1 \\
        1 & 2 & 3 \\
        1 & 4 & 9
      \end{bmatrix}^T
      =
      \begin{bmatrix}
        3 & 6 & 14 \\
        6 & 14 & 36 \\
        14 & 36 & 98
      \end{bmatrix}
      \]
    </div>
    <p>Calculate \( X^T y \):</p>
    <div class="math-block">
      \[
      X^T y =
      \begin{bmatrix}
        1 & 1 & 1 \\
        1 & 2 & 3 \\
        1 & 4 & 9
      \end{bmatrix}
      \begin{bmatrix}
        30000 \\ 50000 \\ 70000
      \end{bmatrix}
      =
      \begin{bmatrix}
        150000 \\
        380000 \\
        980000
      \end{bmatrix}
      \]
    </div>
    <p>
      Now, solve for \( \hat{\beta} \):
    </p>
    <div class="math-block">
      \[
      \hat{\beta} = (X^T X)^{-1} X^T y
      \]
    </div>
    <p>
      Where \( (X^T X)^{-1} \) can be computed numerically (for brevity, not shown here).
    </p>
    <p>
      The final prediction formula for degree 2 polynomial regression is:
    </p>
    <div class="math-block">
      \[
      \hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x + \hat{\beta}_2 x^2
      \]
    </div>
    <p>
      For higher-degree or multi-feature polynomial regression, the matrices expand accordingly, but the process is the same.
    </p>
  </section>

  <section>
    <h2>7. RÂ² Score (Coefficient of Determination)</h2>
    <p>
      \[
      R^2 = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2}
      \]
    </p>
    <p>Where:</p>
    <ul>
      <li>\( \sum (y_i - \hat{y}_i)^2 \): Residual sum of squares (SS<sub>res</sub>)</li>
      <li>\( \sum (y_i - \bar{y})^2 \): Total sum of squares (SS<sub>tot</sub>)</li>
    </ul>
    <p>\( R^2 = 1 \): Perfect fit, \( R^2 = 0 \): No better than mean</p>
  </section>

</body>
</html>